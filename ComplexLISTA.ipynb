{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "4657fdda-780b-41a6-89bc-23a7b3e896e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "c4f57438-6d0e-4c21-8d0a-7a13203fd6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.linalg import norm, pinv\n",
    "from scipy import fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "041d5853-5e92-4af4-86b8-c01a2d33509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import relu, mse_loss\n",
    "from torch.nn import Module, ReLU\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "27e2df59-12ae-49fd-9ee9-af5df29300d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sparse_dataset(Dataset):\n",
    "    def __init__(self, N, K, Nexamples):\n",
    "        self.X = np.zeros((Nexamples, N, 1))\n",
    "        for ii in range(Nexamples):\n",
    "            self.X[ii,...] = self.generate_sparse_vector(N, K)\n",
    "        #self.X *= np.random.randn(*self.X.shape)\n",
    "        self.X = torch.from_numpy(self.X)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i, ...]\n",
    "    \n",
    "    def __len__(self, ):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def generate_sparse_vector(self, N, K):\n",
    "        x = np.zeros((N,1))\n",
    "        x[:K,...] = 1.\n",
    "        np.random.shuffle(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "5ef4b98a-639c-4614-addb-75f6fb5ac7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexLISTA(Module):\n",
    "    \n",
    "    def __init__(self, M, N, maxit):\n",
    "        super(ComplexLISTA, self).__init__()\n",
    "    \n",
    "        # Real and imaginary Wg and We matricies \n",
    "        self.Wre = torch.nn.Parameter(torch.zeros([N, M]), requires_grad=True)\n",
    "        self.Wrg = torch.nn.Parameter(torch.zeros([N, N]), requires_grad=True)\n",
    "        self.Wie = torch.nn.Parameter(torch.zeros([N, M]), requires_grad=True)\n",
    "        self.Wig = torch.nn.Parameter(torch.zeros([N, N]), requires_grad=True)\n",
    "        \n",
    "        # alpha and lambda hyper-parameters to LASSO/ISTA\n",
    "        #self.alpha = torch.nn.Parameter(torch.zeros(maxit), requires_grad=True)\n",
    "        #self.lamda = torch.nn.Parameter(torch.zeros(maxit), requires_grad=True)\n",
    "        self.theta = torch.nn.Parameter(torch.ones(maxit+1), requires_grad=True)\n",
    "        \n",
    "        # Save the passed values\n",
    "        self.M = M\n",
    "        self.N = N\n",
    "        self.maxit = maxit\n",
    "        self.relu = ReLU()\n",
    "\n",
    "        return\n",
    "    \n",
    "    def forward(self, yr, yi):\n",
    "        \n",
    "        Wret = torch.transpose(self.Wre, 0, 1)\n",
    "        Wiet = torch.transpose(self.Wie, 0, 1)\n",
    "        Wrgt = torch.transpose(self.Wrg, 0, 1)\n",
    "        Wigt = torch.transpose(self.Wig, 0, 1)\n",
    "                \n",
    "        # Apply We branch to y to 0-th iteration\n",
    "        zr = torch.matmul(yr, Wret) - torch.matmul(yi, Wiet)\n",
    "        zi = torch.matmul(yi, Wret) + torch.matmul(yr, Wiet)\n",
    "        \n",
    "        # Apply soft-thresholding according to Eldar's paper.\n",
    "        xabs = torch.sqrt(torch.square(zr) + torch.square(zi))\n",
    "        xr = torch.divide(zr, xabs + 1) * self.relu(xabs - self.theta[0])\n",
    "        xi = torch.divide(zi, xabs + 1) * self.relu(xabs - self.theta[0])\n",
    "\n",
    "        #soft = torch.divide(self.theta[0], self.theta[0])\n",
    "        #soft = torch.divide(self.theta[0], torch.max(xabs, self.theta[0]))\n",
    "        #soft = 1 - torch.divide(self.theta[0], relu(xabs - self.theta[0]) + self.theta[0]) \n",
    "        #soft = 1 - torch.divide(self.theta[0], self.relu(xabs - self.theta[0]) + self.theta[0]) \n",
    "        #xr = torch.multiply(zr, softr)\n",
    "        #xi = torch.multiply(zi,  softi)\n",
    "        \n",
    "        \n",
    "        for t in range(1, self.maxit+1):\n",
    "        \n",
    "            # Apply We branch to y to t-th iteration\n",
    "            ar = torch.matmul(yr, Wret) - torch.matmul(yi, Wiet)\n",
    "            ai = torch.matmul(yi, Wret) + torch.matmul(yr, Wiet)\n",
    "            \n",
    "            # Apply Wg branch to x^(t) for t-th iteration\n",
    "            br = torch.matmul(xr, Wrgt) - torch.matmul(xi, Wigt)\n",
    "            bi = torch.matmul(xi, Wrgt) + torch.matmul(xr, Wigt)\n",
    "            \n",
    "            # Add the two branches                                                                           \n",
    "            zr = ar + br\n",
    "            zi = ai + bi\n",
    "            \n",
    "            # Apply soft-thresholding\n",
    "            xabs = torch.sqrt(torch.square(zr) + torch.square(zi))\n",
    "            #xr = zr * torch.pinverse(xabs).t() * self.relu(xabs - self.theta[t])\n",
    "            #xi = zi * torch.pinverse(xabs).t() * self.relu(xabs - self.theta[t])\n",
    "            xr = torch.divide(zr, xabs + 1) * self.relu(xabs - self.theta[t])\n",
    "            xi = torch.divide(zi, xabs + 1) * self.relu(xabs - self.theta[t])\n",
    "            #soft = torch.divide(self.theta[t], self.theta[t]) \n",
    "            #soft = torch.divide(self.theta[t], torch.max(xabs, self.theta[t]))\n",
    "            #soft = 1 - torch.divide(self.theta[t], self.relu(xabs - self.theta[t]) + self.theta[t]) \n",
    "            #soft = 1\n",
    "            #xr = torch.multiply(zr, softr)\n",
    "            #xi = torch.multiply(zi, softi)\n",
    "            \n",
    "            #print(f\"{yr[0,0] = }, {zr[0,0] = }, {xr[0,0] = }, {self.theta[0] = }, {soft[0,0] = }, {self.Wre[0,0] = }, {self.Wrg[0,0] = }\")\n",
    "\n",
    "            if(torch.isnan(zr[0,0])):\n",
    "                print(zr[0,0])\n",
    "                raise Exception('Bad')            \n",
    "        return xr, xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "646a789f-63f6-4e75-b598-ca9f8df702e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.complex128\n"
     ]
    }
   ],
   "source": [
    "# Create ULA and Nested Array Matricies\n",
    "M = 70\n",
    "N = 100\n",
    "sig = 0\n",
    "N1 = M // 2\n",
    "N2 = M - N1\n",
    "\n",
    "inner = np.arange(N1)\n",
    "outer = np.arange(1, N2+1)*(N1 + 1) - 1\n",
    "\n",
    "uniform = np.arange(M).reshape(-1,1)\n",
    "nested = np.concatenate([inner, outer]).reshape(-1, 1)\n",
    "\n",
    "fgrid = fft.fftfreq(N).reshape(-1, 1)\n",
    "\n",
    "complex_exp = lambda x : np.exp(1j* 2*np.pi * x )\n",
    "A_u = complex_exp(uniform @ fgrid.T)\n",
    "A_n = complex_exp(nested @ fgrid.T)\n",
    "\n",
    "A = torch.from_numpy(A_u)\n",
    "print(A.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "596150e5-6a33-4042-89c5-799485cd5850",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxit = 50\n",
    "M = 70\n",
    "N = 100\n",
    "model = ComplexLISTA(M, N, maxit)\n",
    "\n",
    "# for name, weights in model.named_parameters():\n",
    "#     print(name, weights)\n",
    "#print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "a37dbeb5-7183-4320-ac99-6ac6abc5f95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# Create training data\n",
    "epochs = 20\n",
    "batchSize = 10\n",
    "testFreq = 1\n",
    "trainingPoints = 100\n",
    "print(trainingPoints)\n",
    "testingPoints = 100\n",
    "sparsityLevel = 2\n",
    "\n",
    "dataset_training = sparse_dataset(N, sparsityLevel, trainingPoints)\n",
    "dataloader_training = DataLoader(dataset_training, \n",
    "                                 batch_size = batchSize, shuffle=True)\n",
    "dataset_testing = sparse_dataset(N, sparsityLevel, testingPoints)\n",
    "dataloader_testing = DataLoader(dataset_testing, \n",
    "                                 batch_size = 1, shuffle=False)\n",
    "batches = int(trainingPoints/batchSize)\n",
    "training_losslist = np.zeros(epochs * batches)\n",
    "testing_losslist = np.zeros(epochs * batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "8a399404-ab10-4076-b234-6a62a1c46392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Wg matrix according to Eldar\n",
    "X = dataset_training[...][...,0]\n",
    "Y = X @ A_u.T\n",
    "X = X.t().numpy()\n",
    "Y = Y.t().numpy()\n",
    "Phi = Y @ pinv(X.T.conj() @ X) @ X.T.conj() # conjugates omitted since X will be real\n",
    "PhiH = torch.from_numpy(Phi.conj().T)\n",
    "L = np.max(np.abs(np.linalg.eigvals(Phi.conj().T @ Phi)))\n",
    "\n",
    "#print(model.state_dict()['Wre'])\n",
    "with torch.no_grad():\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'Wre' in name:\n",
    "            param.copy_(1/L * PhiH.real)\n",
    "        if 'Wie' in name:\n",
    "            param.copy_(1/L * PhiH.imag)\n",
    "        if 'theta' in name:\n",
    "            param.copy_(0.01/L * torch.ones(model.maxit+1))\n",
    "\n",
    "#print(model.state_dict()['Wre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "6efcd2d6-5443-46eb-9496-c365d6143d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]])\n",
      "tensor([   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
      "           nan,    nan,    nan,    nan,    nan, 0.0108])\n",
      "tensor(nan, grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Bad",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_70566/4081793987.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Send through model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mxpredr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxpredi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxpredr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxpredi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/torch-v1.9.0/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_70566/402532360.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, yr, yi)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Bad'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Bad"
     ]
    }
   ],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=1e-12)\n",
    "obj = torch.nn.MSELoss()\n",
    "for e in range(epochs):\n",
    "    for i, data in enumerate(dataloader_training):\n",
    "\n",
    "        idx = e * batches + i\n",
    "\n",
    "        x = data.numpy().reshape(batchSize, N)\n",
    "        y = torch.from_numpy(x @ A_u.T)\n",
    "        \n",
    "        # Split data to real and complex\n",
    "        xr = data.reshape(batchSize, N).to(torch.float32)\n",
    "        xi = torch.zeros(batchSize, N).to(torch.float32)\n",
    "        \n",
    "        yr = y.real.to(torch.float32)\n",
    "        yi = y.imag.to(torch.float32)\n",
    "        \n",
    "        # Send through model\n",
    "        xpredr, xpredi = model(yr, yi)\n",
    "        \n",
    "        loss = obj(xpredr, xr) + obj(xpredi, xi)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            training_losslist[idx] = loss\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        print(model.Wrg.grad)\n",
    "        print(model.theta.grad)        \n",
    "\n",
    "        optim.step()\n",
    "        \n",
    "        #print(\"Epoch: {}\\t Batch: {}\\t Training Loss: {}\\t\".format(e, i, training_losslist[idx]), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "17f3d02c-87d5-48db-93d9-b9f700d81b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004834003746509552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f983942fbe0>]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(training_losslist[0])\n",
    "plt.plot(training_losslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "629261e3-81f8-4ba7-a3b2-1cbecfb99557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pinverse(torch.tensor([[2.0]]))[0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "43fd5f85-e7da-4294-b9e8-21dbc35fd6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pinverse(torch.tensor([[2.0], [0.0],[0.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf067838-97c3-44d5-976e-7ab7bf006761",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch-v1.9.0]",
   "language": "python",
   "name": "conda-env-torch-v1.9.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
